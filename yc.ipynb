{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32b0bbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68a3a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'skip_download': True,       \n",
    "    'writesubtitles': True,      \n",
    "    'subtitleslangs': ['en'],    \n",
    "    'subtitlesformat': 'vtt',    \n",
    "    'outtmpl': 'subtitle.%(ext)s'\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([video_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19a95bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript saved to  transcript.txt\n",
      "Over the last few years, AI systems have become astonishingly good at turning text props into videos. At the core of how these models operate is a deep connection to physics. This generation of image and video models works using a process known as diffusion, which is remarkably equivalent to the Brownian motion we see as particles diffuse, but with time run backwards, and in high-dimensional space. As we'll see, this connection to physics is much more than a curiosity. We get real algorithms out ...\n"
     ]
    }
   ],
   "source": [
    "vtt_file = \"subtitle.en.vtt\"\n",
    "txt_file = \"transcript.txt\"\n",
    "\n",
    "transcript_lines = []\n",
    "\n",
    "with open(vtt_file,\"r\",encoding=\"utf-8\") as f:\n",
    "  for line in f:\n",
    "    line = line.strip()\n",
    "\n",
    "    if not line or line.startswith(\"WEBVTT\") or \"-->\" in line:\n",
    "      continue\n",
    "    transcript_lines.append(line)\n",
    "\n",
    "transcript = \" \".join(transcript_lines)\n",
    "\n",
    "with open(txt_file,\"w\",encoding=\"utf-8\") as f:\n",
    "  f.write(transcript)\n",
    "\n",
    "print(\"Transcript saved to \",txt_file)\n",
    "print(transcript[:500],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f52f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)\n",
    "chunks = splitter.create_documents([transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6323401e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be3c810e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"Over the last few years, AI systems have become astonishingly good at turning text props into videos. At the core of how these models operate is a deep connection to physics. This generation of image and video models works using a process known as diffusion, which is remarkably equivalent to the Brownian motion we see as particles diffuse, but with time run backwards, and in high-dimensional space. As we'll see, this connection to physics is much more than a curiosity. We get real algorithms out\")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cde648aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = FAISS.from_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55e001f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'b29ff23c-0783-4f65-bef4-28aa8c8c27ad',\n",
       " 1: 'ee71796f-46b5-4c05-a072-b41f87856e6e',\n",
       " 2: 'cfac9cbf-4164-41f2-b616-f9eeaccee6b1',\n",
       " 3: 'c766d092-8735-439b-b2a3-18c54d459803',\n",
       " 4: 'e1f1b1ab-a7f8-4db3-8552-e70dfe43c99b',\n",
       " 5: '87f8a4ff-a0f9-4304-bc4f-d29ce2ad51b5',\n",
       " 6: '2cd65284-a213-476f-ac98-1f546e99dabb',\n",
       " 7: 'b47c963e-a691-4498-9d97-5508e1f7d564',\n",
       " 8: '5730a9b7-dada-4b61-aabf-e9d6df140978',\n",
       " 9: '73633705-d1d2-45d8-93f4-232b7ef1f828',\n",
       " 10: '00027290-cdf3-42a6-bfae-83833e45ba68',\n",
       " 11: 'd7e5b8fd-03eb-431a-ae52-e1c491667102',\n",
       " 12: '30345453-ebe5-4ad3-8b72-91185f128e47',\n",
       " 13: '90baea94-08e3-4b20-a873-9f921d474798',\n",
       " 14: '12fe665c-b356-4a20-a65f-0a32e45f6a6a',\n",
       " 15: '98b7550c-b725-4792-a708-f4829737f8f3',\n",
       " 16: '235b5589-beae-4f1a-8d76-f556f21ccdfd',\n",
       " 17: 'ea19144e-2878-4bb3-a264-a99942279f63',\n",
       " 18: '0d02c0fb-0bfe-4d6e-a4b1-9372dff9fde5',\n",
       " 19: 'ed5991ca-c360-4d2a-b47b-7bc823346ea1',\n",
       " 20: 'd0175a0e-fbf3-4f0a-bdb5-cf743135ece4',\n",
       " 21: '0a6d6d2d-a412-4928-8ea2-848801cd4a54',\n",
       " 22: '3ea01d27-7273-4e4b-9024-67001b8dca68',\n",
       " 23: 'ad0388de-993a-40d2-80ea-2fe0c64ca18b',\n",
       " 24: '46b9b2eb-b2b1-487f-9255-c04d198a0fb8',\n",
       " 25: '1a7aa073-349f-46e1-8391-fd1ccbecd7ce',\n",
       " 26: '4e672f2f-5bbf-44b6-9fe8-796b60c3370a',\n",
       " 27: '76e279f5-2d16-49c3-8048-ddecb42c2135',\n",
       " 28: '7d2c415d-1c02-4711-89d4-78b822dec905',\n",
       " 29: 'f6cb0ac8-27f7-4957-b70e-78eeaf766f6a',\n",
       " 30: 'e25c485a-ffdb-49dc-966a-8fffa162045f',\n",
       " 31: 'f9d254b7-ad00-42d9-adad-005c89405934',\n",
       " 32: '51eb0bab-920e-4b2f-87d1-c47ff08dc1e7',\n",
       " 33: '0f1e8f85-75d5-4279-9e47-eee9ba797b67',\n",
       " 34: '8a3b4489-714e-4e60-a742-5cad15092a6e',\n",
       " 35: '79c2ab2e-106e-4184-be00-509f26d413cf',\n",
       " 36: 'bbf8a354-e500-496d-ab04-53ce602eaad3',\n",
       " 37: '7baea44e-55f4-421f-b901-da05dd72ad00',\n",
       " 38: 'e80a20b1-b9cc-482a-b3b0-1a826725c19e',\n",
       " 39: 'a0aad5d8-d9af-46e2-a824-e2b351f324b3',\n",
       " 40: '7967f7a4-bfae-451d-9068-a44927c5e349',\n",
       " 41: '56e0bf97-08ff-4112-9b1c-65a5306bc8f1',\n",
       " 42: '1aade74d-e815-4c4b-a619-c329830bcfcf',\n",
       " 43: 'bbbcf04b-f573-4204-85e7-3d6b49dfd7ca',\n",
       " 44: '96d6cf8d-201f-4680-a92e-f9ebde2cc0fc',\n",
       " 45: 'da469b67-e1df-4e78-926e-90091a89df2e',\n",
       " 46: 'b181a122-e8f3-4f1b-998b-c2027c2c88fd',\n",
       " 47: 'f95053ce-f0f9-42d3-ad75-f904d145c49e',\n",
       " 48: '2de0e035-2a9f-4893-82f0-49dcba1ed4bf',\n",
       " 49: '35014c6c-9b0e-4a24-98fe-8ff496cdf547',\n",
       " 50: 'e161fb95-0ddb-4813-a2d4-fa2ccfa6a95a',\n",
       " 51: 'feae9ebe-6ba5-4db1-a426-2086c5648220',\n",
       " 52: '4551e0de-bdc1-45cb-96d1-3d1536bf7cf7',\n",
       " 53: '14ab02af-1409-48f1-bf2c-56755bd97ee7',\n",
       " 54: 'd1bd9c40-55e1-49f7-952a-cb46ee4429cd',\n",
       " 55: 'faf0ccf4-ff65-4225-a341-1949d7c5fb4c',\n",
       " 56: '61841adf-bf26-4d5f-b489-f99c63fdcb59',\n",
       " 57: 'ee27ab1b-be55-415e-9026-b543c004762b',\n",
       " 58: 'c920da70-3c43-4be6-a8aa-e468d165f973',\n",
       " 59: '3886b8af-9ac4-4ce8-9bfe-78a7db8d3856',\n",
       " 60: '44dafe48-f13f-41c0-9889-e39797d6af4b',\n",
       " 61: 'bc88fd84-1dc0-403c-ae87-5378c8fcfef7',\n",
       " 62: 'ed3e9f73-a596-48ca-80e5-a48e16a16286',\n",
       " 63: '1199164b-eb6d-4d78-b401-0d5324079776',\n",
       " 64: 'e4e49cc4-0a28-4561-97f3-5d8367333ffa',\n",
       " 65: '431c561b-6034-4406-86d5-d63d4c8dd90c',\n",
       " 66: 'c027c314-922d-4889-952d-305faac696fd',\n",
       " 67: 'ab28d580-8cc3-4902-a3aa-d2d15172d08b',\n",
       " 68: '194b2975-4418-4fd4-b292-3f8ad2a510c5',\n",
       " 69: 'bb8a6c26-8399-4c9b-a952-d87da2c089ad',\n",
       " 70: '2e5cf6d4-ba62-4c47-b810-c5c972679cae',\n",
       " 71: 'a7780b17-38d8-49b0-8e40-be90afc164bd',\n",
       " 72: '36818516-f821-40f2-82be-9418584c743e',\n",
       " 73: 'b540acb5-0b29-4942-a128-ff4760237171',\n",
       " 74: '96a723b5-1495-4312-bc00-3b93fd17ed58',\n",
       " 75: 'c27ecd6c-1a22-41c1-8036-064ff689e42a',\n",
       " 76: '3bf8d2ac-485e-4c4c-9b13-e6e1ed951b9b',\n",
       " 77: 'de24829d-b427-4e69-a29d-ebf30ec7a6b7',\n",
       " 78: '6c15bece-2471-4998-b8a6-2a93e4837a6b',\n",
       " 79: '4d2bf38c-14cb-4eea-b863-0b5b27526815',\n",
       " 80: '301df9af-2f0a-46fd-b1e5-a1b88c32d25b',\n",
       " 81: '2d4375ce-5673-4f57-8ca5-ad7e98a2b2e5',\n",
       " 82: '45079e56-b097-484a-9e82-c1e6e70eff3a',\n",
       " 83: 'd53c903b-9b5f-4a00-863e-d4991430d6c3',\n",
       " 84: '6ded9db9-4fa3-4b72-990e-0aea331337b8',\n",
       " 85: 'f8ff8812-050b-4adf-8c6b-6d752fb30ff5',\n",
       " 86: '86751b39-8561-4604-9220-3e37a39e9ac7',\n",
       " 87: 'd5cb05b4-d539-4dba-b3cb-28ccddde7100',\n",
       " 88: '8c6b68da-4cfd-4c94-b2c7-570724e2e522',\n",
       " 89: 'e1231178-d080-4db0-8910-44a849c58a48',\n",
       " 90: '31a5aa9e-e2e6-4218-8eda-dba6612220e6',\n",
       " 91: 'a542e527-e060-4475-99ef-2743aa58e7d7',\n",
       " 92: 'aadab3fa-099e-4236-a3f8-12ee5ca07748',\n",
       " 93: 'f46fa38f-998c-4de8-872b-9f09ae0418be'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b1e3c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b29ff23c-0783-4f65-bef4-28aa8c8c27ad', metadata={}, page_content=\"Over the last few years, AI systems have become astonishingly good at turning text props into videos. At the core of how these models operate is a deep connection to physics. This generation of image and video models works using a process known as diffusion, which is remarkably equivalent to the Brownian motion we see as particles diffuse, but with time run backwards, and in high-dimensional space. As we'll see, this connection to physics is much more than a curiosity. We get real algorithms out\")]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids(['b29ff23c-0783-4f65-bef4-28aa8c8c27ad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e561402",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef1856be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000273A1BFB0D0>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23f708be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c766d092-8735-439b-b2a3-18c54d459803', metadata={}, page_content=\"model's source code, we'll find that the video generation process begins with this call to a random number generator. Creating a video where the pixel intensity values are chosen randomly. Here's what it looks like. From here, this pure noise video is passed into a transformer. This is the same type of AI model used by large language models, like ChatGPT. But instead of outputting text, this transformer outputs another video that now looks like this. Still mostly noise, but with some hints of\"),\n",
       " Document(id='e1f1b1ab-a7f8-4db3-8552-e70dfe43c99b', metadata={}, page_content=\"outputs another video that now looks like this. Still mostly noise, but with some hints of structure. This new video is added to our pure noise video, and then passed back into the model again, producing a third video that looks like this. This process is repeated again and again. Here's what the video looks like after 5 iterations, 10, 20, 30, 40, and finally 50. Step by step, our transformer shapes pure noise into incredibly realistic video. But what exactly is the connection to Brownian\"),\n",
       " Document(id='d5cb05b4-d539-4dba-b3cb-28ccddde7100', metadata={}, page_content='Of all the interesting details that make these models tick, the most astounding thing to me is that the pieces fit together at all. The fact that we can take a trained text encoder from clip or elsewhere and use its output to actually steer the diffusion process, which itself is highly complex, seems almost too good to be true. And on top of that, many of these core ideas can be built from relatively simple geometric intuitions that somehow hold in the incredibly high dimensional spaces these'),\n",
       " Document(id='0a6d6d2d-a412-4928-8ea2-848801cd4a54', metadata={}, page_content='to generate very high quality images using a diffusion process, where pure noise is transformed step by step into realistic images. The core idea behind diffusion models is pretty straightforward. We take a set of training images and add noise to each image step by step until the image is completely destroyed. From here we train a neural network to reverse this process. When I first learned about diffusion models, I assumed that the models would be trained to remove noise a single step at a')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What are transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "936aad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\",temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4392c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "      You are a helpful assistant.\n",
    "      Answer ONLY from the provided transcript context.\n",
    "      If the context is insufficient, just say you don't know.\n",
    "\n",
    "      {context}\n",
    "      Question: {question}\n",
    "    \"\"\",\n",
    "    input_variables = ['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c46e3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "question          = \"is the topic of transformers discussed in this video? if yes then what was discussed\"\n",
    "retrieved_docs    = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "67eaea84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c766d092-8735-439b-b2a3-18c54d459803', metadata={}, page_content=\"model's source code, we'll find that the video generation process begins with this call to a random number generator. Creating a video where the pixel intensity values are chosen randomly. Here's what it looks like. From here, this pure noise video is passed into a transformer. This is the same type of AI model used by large language models, like ChatGPT. But instead of outputting text, this transformer outputs another video that now looks like this. Still mostly noise, but with some hints of\"),\n",
       " Document(id='e1f1b1ab-a7f8-4db3-8552-e70dfe43c99b', metadata={}, page_content=\"outputs another video that now looks like this. Still mostly noise, but with some hints of structure. This new video is added to our pure noise video, and then passed back into the model again, producing a third video that looks like this. This process is repeated again and again. Here's what the video looks like after 5 iterations, 10, 20, 30, 40, and finally 50. Step by step, our transformer shapes pure noise into incredibly realistic video. But what exactly is the connection to Brownian\"),\n",
       " Document(id='5730a9b7-dada-4b61-aabf-e9d6df140978', metadata={}, page_content=\"really work in practice and give us some powerful theory for dramatically speeding up image and video generation. Finally, we'll bring these worlds together and see how approaches like CLIP are combined with diffusion models to condition and guide the generation process towards the videos we ask for in our prompts. 2020 was a landmark year for language modeling. New results in neural scaling laws and OpenAI's GPT-3 showed that bigger really was better. Massive models trained on massive datasets\"),\n",
       " Document(id='b29ff23c-0783-4f65-bef4-28aa8c8c27ad', metadata={}, page_content=\"Over the last few years, AI systems have become astonishingly good at turning text props into videos. At the core of how these models operate is a deep connection to physics. This generation of image and video models works using a process known as diffusion, which is remarkably equivalent to the Brownian motion we see as particles diffuse, but with time run backwards, and in high-dimensional space. As we'll see, this connection to physics is much more than a curiosity. We get real algorithms out\")]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac742413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model's source code, we'll find that the video generation process begins with this call to a random number generator. Creating a video where the pixel intensity values are chosen randomly. Here's what it looks like. From here, this pure noise video is passed into a transformer. This is the same type of AI model used by large language models, like ChatGPT. But instead of outputting text, this transformer outputs another video that now looks like this. Still mostly noise, but with some hints of\\n\\noutputs another video that now looks like this. Still mostly noise, but with some hints of structure. This new video is added to our pure noise video, and then passed back into the model again, producing a third video that looks like this. This process is repeated again and again. Here's what the video looks like after 5 iterations, 10, 20, 30, 40, and finally 50. Step by step, our transformer shapes pure noise into incredibly realistic video. But what exactly is the connection to Brownian\\n\\nreally work in practice and give us some powerful theory for dramatically speeding up image and video generation. Finally, we'll bring these worlds together and see how approaches like CLIP are combined with diffusion models to condition and guide the generation process towards the videos we ask for in our prompts. 2020 was a landmark year for language modeling. New results in neural scaling laws and OpenAI's GPT-3 showed that bigger really was better. Massive models trained on massive datasets\\n\\nOver the last few years, AI systems have become astonishingly good at turning text props into videos. At the core of how these models operate is a deep connection to physics. This generation of image and video models works using a process known as diffusion, which is remarkably equivalent to the Brownian motion we see as particles diffuse, but with time run backwards, and in high-dimensional space. As we'll see, this connection to physics is much more than a curiosity. We get real algorithms out\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b5563ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({\"context\":context_text,\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1ee6bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"\\n      You are a helpful assistant.\\n      Answer ONLY from the provided transcript context.\\n      If the context is insufficient, just say you don't know.\\n\\n      model's source code, we'll find that the video generation process begins with this call to a random number generator. Creating a video where the pixel intensity values are chosen randomly. Here's what it looks like. From here, this pure noise video is passed into a transformer. This is the same type of AI model used by large language models, like ChatGPT. But instead of outputting text, this transformer outputs another video that now looks like this. Still mostly noise, but with some hints of\\n\\noutputs another video that now looks like this. Still mostly noise, but with some hints of structure. This new video is added to our pure noise video, and then passed back into the model again, producing a third video that looks like this. This process is repeated again and again. Here's what the video looks like after 5 iterations, 10, 20, 30, 40, and finally 50. Step by step, our transformer shapes pure noise into incredibly realistic video. But what exactly is the connection to Brownian\\n\\nreally work in practice and give us some powerful theory for dramatically speeding up image and video generation. Finally, we'll bring these worlds together and see how approaches like CLIP are combined with diffusion models to condition and guide the generation process towards the videos we ask for in our prompts. 2020 was a landmark year for language modeling. New results in neural scaling laws and OpenAI's GPT-3 showed that bigger really was better. Massive models trained on massive datasets\\n\\nOver the last few years, AI systems have become astonishingly good at turning text props into videos. At the core of how these models operate is a deep connection to physics. This generation of image and video models works using a process known as diffusion, which is remarkably equivalent to the Brownian motion we see as particles diffuse, but with time run backwards, and in high-dimensional space. As we'll see, this connection to physics is much more than a curiosity. We get real algorithms out\\n      Question: is the topic of transformers discussed in this video? if yes then what was discussed\\n    \")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7076721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Yes, transformers are discussed in this video. The video generation process involves passing a pure noise video into a transformer, which is the same type of AI model used by large language models like ChatGPT. However, instead of outputting text, this transformer outputs another video. This new video is added to the original pure noise video and then passed back into the model again. This process is repeated multiple times, with the transformer gradually shaping the pure noise into a realistic video.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 466, 'total_tokens': 561, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-CRjCMlkLDHDdA1TWB9XBa1BMn39wn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--2db48546-c2f4-4554-8d69-4f14ce3601c4-0' usage_metadata={'input_tokens': 466, 'output_tokens': 95, 'total_tokens': 561, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(final_prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0f1d4",
   "metadata": {},
   "source": [
    "### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "004e2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough,RunnableLambda \n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "88415c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "  return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c6eb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "  'context' : retriever | RunnableLambda(format_docs),\n",
    "  'question' : RunnablePassthrough() \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6807349b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"text or images into embedding vectors. These two problems potentially fit together in a really interesting way. Diffusion models are able to potentially reverse the CLIP image encoder, generating high quality images, and the output vector of the CLIP text encoder could be used to guide our diffusion models toward the images or videos that we want. So the high level idea here is that we could pass in a prompt into the CLIP text encoder to generate an embedding vector, and use this embedding\\n\\nspace allows us to operate mathematically on the pure ideas or concepts in our images and text, translating the differences in the content of our images, like if there's a hat or not, into a literal distance between vectors in our embedding space. The OpenAI team showed that CLIP could produce very impressive image classification results by simply passing an image into our image encoder, and then comparing the resulting vector to a set of possible captions, one for each label that could be\\n\\nimage and text to our shared embedding space. We have no way of generating images and text from our embedding vectors. 2020 turned out not only to be a transformative year for language modeling. A few weeks after the GPT-3 paper came out, a team at Berkeley published a paper called Denoising Diffusion Probabilistic Models, now known as DDPM. The paper showed for the first time that it was possible to generate very high quality images using a diffusion process, where pure noise is transformed\\n\\nan unprecedented level of detail from the input text. The team called their method unCLIP, but their model is better known by its commercial name, DALI2. But how do we actually use the embedding vectors for models like CLIP to steer the diffusion process? One option is to simply pass our text vector as another input into our diffusion model, and train as we normally would to remove noise. If we train our diffusion model using image and caption pairs, as the OpenAI team did, the idea here is\",\n",
       " 'question': 'What are embeddings'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke('What are embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1736c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92b4a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chain = parallel_chain | prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa7ae5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The video discusses the process of creating realistic video from pure noise using a transformer model. This process involves multiple iterations of adding a new video to the noise video and passing it back into the model. The video also discusses the use of negative prompts to steer the diffusion process away from unwanted features. The model uses a text input to shape the noise into what the prompt describes. The video also mentions a 2021 OpenAI paper and model called CLIP, which is a combination of a language model and a vision model.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke('Can you summarize this video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a943d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
